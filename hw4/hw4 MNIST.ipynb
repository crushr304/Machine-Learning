{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-5bf6394e0167>:33: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\GSS-fearless\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\GSS-fearless\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\GSS-fearless\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\GSS-fearless\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\GSS-fearless\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-1-5bf6394e0167>:151: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-1-5bf6394e0167>:169: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "step:199, loss:0.103400, train_acc:0.970000, test_acc:0.968700\n",
      "step:399, loss:0.138889, train_acc:0.970000, test_acc:0.977600\n",
      "step:599, loss:0.095614, train_acc:0.990000, test_acc:0.982800\n",
      "step:799, loss:0.134975, train_acc:0.970000, test_acc:0.984200\n",
      "step:999, loss:0.063031, train_acc:1.000000, test_acc:0.987300\n",
      "step:1199, loss:0.027513, train_acc:1.000000, test_acc:0.986900\n",
      "step:1399, loss:0.024780, train_acc:1.000000, test_acc:0.988300\n",
      "step:1599, loss:0.029398, train_acc:0.990000, test_acc:0.988700\n",
      "step:1799, loss:0.057245, train_acc:0.990000, test_acc:0.989700\n",
      "step:1999, loss:0.047942, train_acc:0.990000, test_acc:0.989900\n",
      "step:2199, loss:0.043584, train_acc:0.980000, test_acc:0.988700\n",
      "step:2399, loss:0.047375, train_acc:1.000000, test_acc:0.988800\n",
      "step:2599, loss:0.028231, train_acc:1.000000, test_acc:0.991500\n",
      "step:2799, loss:0.017219, train_acc:1.000000, test_acc:0.991200\n",
      "step:2999, loss:0.016216, train_acc:1.000000, test_acc:0.989100\n",
      "step:3199, loss:0.016055, train_acc:1.000000, test_acc:0.992100\n",
      "step:3399, loss:0.015905, train_acc:1.000000, test_acc:0.992000\n",
      "step:3599, loss:0.015130, train_acc:1.000000, test_acc:0.990200\n",
      "step:3799, loss:0.019130, train_acc:1.000000, test_acc:0.988600\n",
      "step:3999, loss:0.048817, train_acc:1.000000, test_acc:0.990000\n",
      "step:4199, loss:0.018592, train_acc:1.000000, test_acc:0.991000\n",
      "step:4399, loss:0.011366, train_acc:1.000000, test_acc:0.990700\n",
      "step:4599, loss:0.010688, train_acc:1.000000, test_acc:0.991900\n",
      "step:4799, loss:0.033235, train_acc:1.000000, test_acc:0.992100\n",
      "step:4999, loss:0.012139, train_acc:1.000000, test_acc:0.990000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"A very simple MNIST classifier.\n",
    "\n",
    "See extensive documentation at\n",
    "\n",
    "https://www.tensorflow.org/get_started/mnist/beginners\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    " \n",
    "\n",
    "import argparse\n",
    "\n",
    "import sys\n",
    "\n",
    " \n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    " \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "data_dir = './data/'\n",
    "\n",
    "mnist = input_data.read_data_sets(data_dir, one_hot=True)\n",
    "\n",
    "#第一层卷积层尺寸和深度\n",
    "\n",
    "CONV_1_SIZE = 3    \n",
    "\n",
    "CONV_1_DEEP = 32  \n",
    "\n",
    "INPUT_CHANNELS = 1 #输入通道数\n",
    "\n",
    " \n",
    "\n",
    "#第二层卷积层尺寸和深度\n",
    "\n",
    "CONV_2_SIZE = 3\n",
    "\n",
    "CONV_2_DEEP = 64\n",
    "\n",
    " \n",
    "\n",
    "#每批次数据集的大小\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    " \n",
    "\n",
    "#学习率\n",
    "\n",
    "LEARNING_RATE_INIT = 1e-3    #学习率初始值\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    " \n",
    "\n",
    "#对输入向量x转换成图像矩阵形式\n",
    "\n",
    "with tf.variable_scope('reshape'):\n",
    "\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1]) #因为数据的条数未知,所以为-1\n",
    "\n",
    " \n",
    "\n",
    "#卷积层1\n",
    "\n",
    "with tf.variable_scope('conv1'):\n",
    "\n",
    "    initial_value = tf.truncated_normal([CONV_1_SIZE,CONV_1_SIZE,INPUT_CHANNELS,CONV_1_DEEP], stddev=0.1)\n",
    "\n",
    "    conv_1_w = tf.Variable(initial_value=initial_value, collections=[tf.GraphKeys.GLOBAL_VARIABLES, 'WEIGHTS'])\n",
    "\n",
    "    conv_1_b = tf.Variable(initial_value=tf.constant(0.1, shape=[CONV_1_DEEP]))\n",
    "\n",
    "    conv_1_l = tf.nn.conv2d(x_image, conv_1_w, strides=[1,1,1,1], padding='SAME') + conv_1_b\n",
    "\n",
    "    conv_1_h = tf.nn.relu(conv_1_l)\n",
    "\n",
    " \n",
    "\n",
    "#池化层1\n",
    "\n",
    "with tf.variable_scope('pool1'):\n",
    "\n",
    "    pool_1_h = tf.nn.max_pool(conv_1_h, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "\n",
    " \n",
    "\n",
    "#卷积层2\n",
    "\n",
    "with tf.variable_scope('conv2'):\n",
    "\n",
    "    conv_2_w = tf.Variable(tf.truncated_normal([CONV_2_SIZE,CONV_2_SIZE,CONV_1_DEEP,CONV_2_DEEP], stddev=0.1),\n",
    "\n",
    "                           collections=[tf.GraphKeys.GLOBAL_VARIABLES, 'WEIGHTS'])\n",
    "\n",
    "    conv_2_b = tf.Variable(tf.constant(0.1, shape=[CONV_2_DEEP]))\n",
    "\n",
    "    conv_2_l = tf.nn.conv2d(pool_1_h, conv_2_w, strides=[1,1,1,1], padding='SAME') + conv_2_b\n",
    "\n",
    "    conv_2_h = tf.nn.relu(conv_2_l)\n",
    "\n",
    " \n",
    "\n",
    "#池化层2\n",
    "\n",
    "with tf.name_scope('pool2'):\n",
    "\n",
    "    pool_2_h = tf.nn.max_pool(conv_2_h, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "\n",
    " \n",
    "\n",
    "#全连接层1\n",
    "\n",
    "with tf.name_scope('fc1'):\n",
    "\n",
    "    #\n",
    "\n",
    "    fc_1_w = tf.Variable(tf.truncated_normal([7*7*64, 1024], stddev=0.1))\n",
    "\n",
    "    fc_1_b = tf.Variable(tf.constant(0.1, shape=[1024]))\n",
    "\n",
    "    #全连接层的输入为向量,而池化层2的输出为7x7x64的矩阵,所以这里要将矩阵转化成一个向量\n",
    "\n",
    "    pool_2_h_flat = tf.reshape(pool_2_h, [-1,7*7*64])\n",
    "\n",
    "    fc_1_h = tf.nn.relu(tf.matmul(pool_2_h_flat, fc_1_w) + fc_1_b)\n",
    "\n",
    "    \n",
    "\n",
    "#dropout在训练时会随机将部分节点的输出改为0,以避免过拟合问题,从而使得模型在测试数据上的效果更好\n",
    "\n",
    "#dropout一般只在全连接层而不是卷积层或者池化层使用\n",
    "\n",
    "with tf.name_scope('dropout'):\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    fc_1_h_drop = tf.nn.dropout(fc_1_h, keep_prob)\n",
    "\n",
    "    \n",
    "\n",
    "#全连接层2 And 输出层\n",
    "\n",
    "with tf.name_scope('fc2'):\n",
    "\n",
    "    fc_2_w = tf.Variable(tf.truncated_normal([1024,10], stddev=0.1), collections=[tf.GraphKeys.GLOBAL_VARIABLES, 'WEIGHTS'])\n",
    "\n",
    "    fc_2_b = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "\n",
    "    y = tf.matmul(fc_1_h_drop, fc_2_w) + fc_2_b\n",
    "\n",
    "    \n",
    "\n",
    "#交叉熵\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    " \n",
    "\n",
    "#l2正则项\n",
    "\n",
    "l2_loss = tf.add_n([tf.nn.l2_loss(w) for w in tf.get_collection('WEIGHTS')])\n",
    "\n",
    " \n",
    "\n",
    "#代价函数 = 交叉熵加上惩罚项\n",
    "\n",
    "total_loss = cross_entropy + 7e-5*l2_loss\n",
    "\n",
    " \n",
    "\n",
    "#定义一个Adam优化器\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE_INIT).minimize(total_loss)\n",
    "\n",
    " \n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "sess.run(init_op)\n",
    "\n",
    " \n",
    "\n",
    "#Train\n",
    "\n",
    "for step in range(5000):\n",
    "\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "\n",
    "    _, loss, l2_loss_value, total_loss_value = sess.run(\n",
    "\n",
    "        [train_step, cross_entropy, l2_loss, total_loss],\n",
    "\n",
    "        feed_dict={x: batch_xs, y_:batch_ys, keep_prob:0.5})\n",
    "\n",
    "    \n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #\n",
    "\n",
    "    if (step+1)%200 == 0:\n",
    "\n",
    "        #每隔200步评估一下训练集和测试集\n",
    "\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_xs, y_:batch_ys, keep_prob:1.0})\n",
    "\n",
    "        test_accuracy = accuracy.eval(feed_dict={x:mnist.test.images, y_:mnist.test.labels, keep_prob:1.0})\n",
    "\n",
    "        print(\"step:%d, loss:%f, train_acc:%f, test_acc:%f\" % (step, total_loss_value, train_accuracy, test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
